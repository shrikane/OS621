CMSC 621 Erlang Project Report
Authors: Yin Huang, Anuja Kench, Shrinivas Kane, Abhishek Sethi
Date: May-10-2014

Section I Introduction to Distributed Programming 
The ability for communication, computation, and replication of data across multiple machines has been a key area of computer science for years. This type of a network, where machines communicate via messages to complete some kind of computation is known as a distributed system. A distributed system is defined as a collection of independent computers that appear to its user as a single coherent system (Tanenbaum). One main advantage this type of networking presents is the ability to take computationally intensive tasks, and spread the load across multiple machines so no one machine must perform all the work. When discussing distributed systems, it is common to talk about latency and scalability. Latency is the time difference from when the last bit of a request is sent, and the first bit of the response is received. Based on the definition of a distributed system, the goal would be to minimize latency so that the user that sends the request does not have to wait too long. The user wants a response almost immediately. Along with a low latency, the ideal distributed system would provide high accuracy when performing computation. The user wants the most up to date information from the system, and accurate results from any computations performed. 
Scalability in a distributed system is measured in 3 different ways (Tanenbaum). The first is respect to size, as in how many more users and resources are added to the system. The second measure is in terms of geographic scalability. In geographic scalability, resources and users can lie far apart physically, but to the user, it appears as of the resources is nearby since it is accessed rapidly. The final measure is that of administrative scalability. In administrative scalability, the distributed system can be managed easily even if it spans across multiple independent administrative organizations. Scalability is an important concept in distributed systems, but more important is the various types of transparency that a distributed system offers. Transparency in a distributed setting is when low level tasks, and various details of the distributed system are hidden from the user. The various types of transparency include access, location, migration, relocation, replication, concurrency, and failure. Access refers to hiding the differences in underlying data representation. Location is the secrecy of where the resource is located relative to the user. Migration and relocation refer to hiding when a resource is moved to another location, the latter while the resource is in use. Replication is hiding that a resource lies across multiple machines. Concurrency is hiding the fact that other users are also using various resources, and failure is hiding when resources fail to the user. These various types of transparency are ideal goals of a distributed system that help improve the user experience when they use the system. 
Though distributed systems make computations easier, and help to provide various qualities that a single machine system cannot, there are pitfalls and complications to a distributed system. The first being providing all types of distribution transparency is difficult to do without sacrificing for either scalability and/or latency. The network upon which the system lies on also complicates things. Since networks are not reliable no matter what, the network bandwidth and communication costs can have an effect on message passing in a distributed system. The distributed system essentially comes down to how quickly can messages be passed from machine to machine, minimizing latency as much as possible. Issues with security also arise. Messages should not be sent out in the open, but rather encrypted to protect data confidentiality and integrity. However, the computation for encryption and decryption should not be very intensive on the machines, meaning an appropriate crypto system must be implemented. Finally, the heterogeneity of the machines on the system is important. Different operating systems, and different types of machines will represent data differently, and this problem must be addressed in a successful distributed system. Distributed systems provide excellent ways to solve computationally intensive problems, but they also come with their own set of issues and difficulties. 

Section II Introduction to the Gossip Algorithm
Having machines communicating and networked is the first step of establishing a distributed system, but once all the machines are connected, then some sort of communication protocol must be used across the various machines. One idea is the broadcast a message from a single machine (machines will be referred to as nodes from here on), to all other machines. The problem here is that too many messages are being sent out, resulting in a large amount of bandwidth being used up on the network. A solution to this is the gossip algorithm. Gossiping amongst humans is when one person is told something, and then that person passes the message onwards to everyone they know, who passes it on to everyone they know, and so on and so forth. Gossip in a distributed system follows the same underlying idea. Gossiping in a distributed system is known as an epidemic algorithm (Demers). This means that the message “infects” nodes that receive the message, and the infected nodes pass the message onwards to their neighbors. The basic idea to gossip follows the same idea as human gossiping, but the difference is that when a node in a distributed system receives a message, it randomly selects one of its neighbors, and begins communication with that neighbor. As long as those two neighbors are communicating, neither one of them can communicate with another neighbor, so communication is locked between the two. Gossip in a distributed system is this simple, and fairly easy to implement. One advantage this provides is that nodes will have fast communication because they pick their neighbors, not far away nodes. Computation also converges relatively quickly with gossiping, decreasing latency while still making use of the distributed nature of the system. Along with the advantages, there are a couple of disadvantages. One being accuracy is not guaranteed in a gossiping system. If a file is replicated across 5 nodes, and an update message is gossiped out, if a user reads one of the replicas before the node receives the update message, then the user is getting old data, which is not desirable behavior. This can be addressed with mutual exclusion, but that may not even remedy the situation if the read is successfully done before the write message even comes. Another disadvantage is that since neighbors are randomly chosen to gossip, it is possible that a neighbor is chosen multiple times in a row. This would cause the same message to be sent over and over again. With high communication latency costs from the network, this would be repetitive to do. With small messages this is not an issue, but for larger ones it could lead to latency problems. Though there are disadvantages to the gossiping algorithm, the advantages outweigh the disadvantages, making gossiping the algorithm of choice for the project presented here. 

Section III Introduction to Erlang
There are many ways to program a distributed application. The first programming languages that come to mind are Java, C, Ruby, etc, basically languages that allow for thread/process creation and maintenance. In this application, the language of choice was Erlang. Erlang is a functional programming language, specifically created to develop distributed applications (Armstrong). Erlang is known for use in building massively parallel distributed applications strictly because the processes in Erlang are very lightweight, allowing for millions to run efficiently on multi core processors (Armstrong). The reason these processes are light weight is because there is no shared memory, just everything put together. This can be viewed as an advantage or disadvantage depending on the level of control the programmer would like. Though Erlang seems like a great language, there are some issues with it. One such issue is that the learning curve for Erlang can be quite large if the programmer has no functional programming experience. Another issue is the limited amount of support for Erlang in the community. There are plenty of forums, but there is still a limited amount of information on the solutions to various problems encountered during programming. Finally, Erlang’s error messages are not very useful, just like other functional programming languages, making debugging harder compared to other languages. There are many other aspects of Erlang that are not mentioned here, but overall Erlang seems to be a good language of choice for building distributed applications.

Section IV Application Introduction
The purpose of this application is to develop a distributed system using Erlang that carries out simple tasks across various nodes. The application took in a file (F), and then segmented the file into M segments (F_1, F_2, etc), and was assigned to N nodes. F contained a list of floating point numbers, so each fragment contained a set of floating point numbers that were distributed across the nodes. The various tasks that were to be performed across the system are the minimum, maximum, and average value of the floating point numbers in F. Also, reading and writing to specific fragments were to be supported as well. Since the original file was fragmented amongst the nodes, some way of retrieval was required to achieve accurate results. This is where the gossip algorithm was used for communication between nodes so that appropriate values could be discovered. The application that was designed made the tradeoff of accuracy for decreased latency, meaning latency is of more importance than 100% accurate answers. The specific implementation presented here takes into account decreased latency and increased accuracy. The details of the implementations are provided below.
Along with the increased accuracy for decreased latency, our application supports location, migration, and replication transparency. Location transparency is provided because the user does not know where specific fragments are located, all that is known is that the fragment has been read/written to, while the Erlang code takes care of the rest. Migration transparency is provided because the file fragments can be moved when not in use, but due to the nature of the gossip algorithm, the exact location will not matter as the computations will eventually converge at some point in time. Replication transparency is taken care of by taking copies of the file fragments, and assigning them to each node as well. Each replica gets updated when a write request comes in. This replication is what may cause decreased accuracy. If one replica does not get updated in time, but a read request comes in before the replica receives the write request, the read request is served first in the application. Unfortunately, this specific case was not addressed in the application, but will be in future work. On the other hand, read after write successfully executes so that read statements received at the same time as write statements are done after the writes have been completed to provide mutual exclusion. This distributed application successfully carries out the computations described above, and provides the various types of transparency as well.

Section V Implementation Overview 
Our implementation of Gossip-based aggregation and data update and retrieval in Erlang aims at performing the following four computational tasks as required in the project description. 
A single large file F with floating-point numbers have been split into M fragments F_1, F_2, …, F_M which have been placed at various nodes of the system. Each fragment is placed at one or more nodes. 
1.	Compute the minimum (min) and maximum (max) value in F and store them at node 1. 
2.	Compute the average (avg) of the values in F and store it at all the nodes.
3.	Update the contents of fragment i at each node that may have a copy of it. 
4.	Retrieve the contents of fragment I from any node that may have an up-to-date copy of it. 
Two types of network were simulated between two virtual machines hosted on two different laptops:
a.	Ring network
b.	Meshed network
In our implementation, we define a node as an Erlang process with process name P_# and each node stores its fragments F_#.  Every node maintains a neighbor node list. This list differs as in these two types of networks. However the gossip mechanism are the same. A computational task request is initialized at node 1, and node 1 will start spreading out the request to its neighbors. Whenever a neighbor node receives a request, this node will first exchange information with the requesting node, and then spread the request to its neighbors.  In such a way this request will eventually reach out to the whole network. In order to pick a neighbor, we use a random number generator to pick a node from the neighbor list. Meanwhile, the user can specify the number of iterations for every node to send out the gossip.
Assumptions we have made:
1.	The network is static, which means we don't deal with the node failure, node participation or node departure. We assume every node is alive and stable.  
2.	The network is reliable and there is no message loss. 
3.	
The report is organized in the following sections: Section VI details the network topologies used, Section VII explains the gossip algorithm, Section VIII focuses on the min, max, avg, read and write operation. Section IX demonstrates the research results. Section X is the analysis and discussion, followed by conclusion and future work in Section XI and Section XII.  

Section VI Network Topologies 
2 different networks were developed for the application. The first of which is a typical ring network. Figure 1 shows a diagram of the network. 
%FIGURE 1 IS IN THE GITHUB REPO%
Figure 1: Diagram of the ring network. An example of the filled in data structures for node 2 would be the neighbors are [1, 3], process name is P_2, fragments would be [F_2, F_5, F_1], and then the functions for min, max, average, read, write are found in the last part. 

The ring topology is a static topology, and assumes that no nodes have failed or caused an exception. Each node contains the listed variables/lists shown in figure 1. This information is still light weight in terms of memory usage, allowing the processes in Erlang to remain light weight. The key here is that each node only has 2 neighbors to gossip with, drastically reducing the number of neighbors to choose from compared to the MESH network. This makes communication quick and effective. The problem here would be if for a large ring, the file that is needed for reading is on the other side of the ring. For example, if the entry point is node 4, but the file resides in node 2, and if 4 randomly gossips with node 5, then 5 gossips with 1, and 1 gossips with 2, we finally reach our destination. For this small of a ring, it is not an issue, but when the ring reaches sizes of 10,000 and more, latency increases. The ability to have 2 neighbors is beneficial, but it also may cause increases in latency in worst case scenarios. 
The MESH network is similar to the ring network, except in the MESH network, each node is connected to each other, meaning each node is assumed to know about every other node in the network. Figure 2 diagrams the MESH network used here. 
%FIGURE 2 IS IN THE GITHUB REPO%
Figure 2: The MESH network used in the application. The only difference from Figure 1 in terms of data saved is that there are more neighbors now. For example, node 2’s neighbors would be [1, 3, 4, 5] now. 

The key in the MESH network is that each node supposedly knows the existence of all the other nodes in the network. It is also a static topology, just like the ring network. The issue here would be that during gossip, latency would increase greatly with the number of nodes. In the worst case scenario, since a random neighbor is chosen to gossip with, if that neighbor does not end up being the node that contains the desired fragment, it is possible that the message passes across all the nodes in the network before it reaches the desired destination. Other than this, the MESH network proved to be decent for scalability as shown in the results. 
The key idea with both topologies is that they are overlay networks. These overlay networks reside on multiple physical machines. For future reference, the physical machines will be called network machines, and each node is represented as an Erlang process in the application. In order for true distribution, these nodes were not spawned on a single network machine. Instead the nodes were created across multiple network machines. In order to ensure an even split of the nodes, thus hopefully distributing the load evenly, the nodes were assigned based on modulo arithmetic. If there are 3 network machines, the nodes were divided evenly through the 3 machines based on the (node number) modulo (total number of network machines), and 1 was added to the result for 1-based indexing purposes. For example, node 2 would reside on machine 3 in the network (2 mod 3 is 2, 2+1 is 3), while node 4 would reside on network machine 2 (4 mod 3 is 1, 1+1 is 2). Each node is programmed to appear as if they all reside on the same machine to themselves and the user, hence the overlay network, but instead they are distributed amongst various machines. Normally, if a node is on a different machine, it would be difficult to send a message to it, but Erlang makes that easy as well. Erlang allows for nodes to be registered on a global registry as opposed to the local registry. As long as the network machines are connected, then the global registry for the network machines will match up. By registering nodes globally, this allows each node to access any other node in the network as long as it knows its node ID. So if node 2 wanted to access node 3 (both are on machines 3 and 1 respectively), node 2 could just send a message via the global registry to node 3 without a problem. The overlay networks presented here helped to create the basic layout for the distributed application, each with their own advantages and disadvantages. 

Section VII Gossip algorithm in the Application 
In large distributed, heterogeneous networks, applications are asking for a set of functions that provide components of a distributed system access to global information such as network size, average load etc. Gossip-based aggregation protocol works in a fully decentralized manner where all nodes exchange the local information with its neighbors and thus infer the global information. There are two types of protocols, reactive and proactive protocols. We focus on the proactive protocol which pushes the query from any issuer to all nodes in the system.   
We use asynchronous push-pull gossip protocol in our implementation. The task of a proactive protocol is to continuously provide all nodes with an up-to-date information held by current set of nodes. Two services are provided in each node, one is to receive request, the other go update information and send out the new information. 
Every node maintains a neighbor list as its neighbor nodes. Whenever a node receives a request message, it will randomly pick up a node in the neighbor list, and forward the request. We set up a threshold parameter named Itr to determine the number of iterations. So the gossip for each node will be executing Itr times. We will report our convergence ratio in our result section with regard to the size of the network, topology of the network, and the size of the file, number of segmentation of the file etc. 

Section VIII 
A.	Implementation of minimum, maximum and average calculation:
We have designed a push-pull model for calculating minimum value, maximum value and average of all the values present at all the nodes in the system.
We have implemented the following algorithm for computing the minimum and maximum value in a large file containing floating point numbers:
When ever a node is initialised, it computes local minimum / local maximum value contained in its fragments as a part of initialisation routine. 
For computing minimum value in the system, each and every node selects a random node from its list of neighbours and pushes its local minimum value to it. It then pulls the local minimum value from the other node and recomputes its new local minimum value as the smallest of the two values. Similarly strategy is applied for calculating the maximum value. The only difference is that each and every node updates its local maximum value as the largest value of the the two local maximum values.
For average calculation, as a part of its initialising routine, each and every node computes local sum as well as local count of the number of values is contains in its fragments. A node randomly selects a node from its neighbour list and they exchange their local sum and local count. Both of them, recompute their local sum as aggregation of both the local sums. Similarly each one of them update their local count. We claim that over a period of time the ratio of local sum and local count at every node remains same after a period of time and is it gives the average of the values in the fragments at that node.
B.	 Write and read segmentation implementation
We have wrapped both the read and write operation in a single process called Update(). The reason to put these two operations in the same process is to avoid the problems incurred by multiple accesses to the same file. Update() will only execute either write or read to the segmentation. Since Erlang has the FIFO mailbox, we simply utilize this mailbox as a job request queue. 
Whenever a process receives a write or read message, the process will check its own data directory to see if it contains the target segmentation. For write operation, if the process contains the segmentation, it will continue and update the segmentation, else it will pick a random neighbor and forward the request. For read operation, if the process contains the segmentation, it will reply the values of the segmentation to the request node, otherwise it will forward the request to its neighbor. We have two operation modes: a. synchronized b. non-synchronized. In Synchronized case, the request node will wait for all replies for target nodes until it stops gossiping. In non-synchronized case, the request node will not wait for any replies but rather work on other things. The synchronized case gives better consistency while the non-synchronized has better performance (time response). The former is more complicated than the latter.
In our implementation, we take the non-synchronized mode where the write request is sent out and gossip a fixed number of iterations while the read request is only waiting for the first reply from any matching node that has the segmentation.
The advantage of such design is that we can guarantee that no race condition for the file will occur, hence a higher accuracy. The disadvantage is the response latency because every time only one node is allowed to do operation on the fragmentation. 
Another problem with this design is that we are not able to guarantee that a read request will always get the latest version, current implementation only accepts the first response without checking the replies from other nodes. Solution to this problem comes in the following directions. First, we could maintain a list of replicas for each fragmentation, when update the fragmentation, we send out three update requests to these nodes and wait for acknowledgement. We call this schema write-to-all and read-once. The problem with this solution is the high latency for the update operation. Also we need global information about the nodes that hold the replicas. Second, we could use a vector to record the number of updates for each fragmentation, and read the reply with the highest number.  
Section IV Results 

Section IX Analysis and discussion

%%Shrinivas



Graph 1 : http://jsfiddle.net/DrVD2/2/
Graph 2 : http://jsfiddle.net/yMLFn/2/
Graph 3 : http://jsfiddle.net/62Rgw/1/
 

Experiment analysis Results:
We conducted experiments on Ring and fully connected (Mesh) topology with different parameters.Following graph shows output of experiments using following configration.
a. Varied number of nodes from 10 to 2000
b. Used 3000 random floating point number

Firstly, number of messages sent while gossiping is approximately same for minimum, maximum, average computation for specific topology.
Secondly, Average calculation in ring topology took long time compared to mesh topology even for 100 nodes. Additionally in average calculation error factor goes on increasing as number of nodes goes on increasing.
Finally, due to randomness in node selection in gossip converged value for average computation may differ in each round.

Please feel free to add or edit results :)
%% end Shrinivas


Problem 1 Consistency
Solution: a. 
Section VI Conclusion
In this project, we have successfully implemented Gossip-based aggregation and data update and retrieval for a large distributed system using Erlang. The three computational tasks have been demonstrated in terms of efficiency, effectiveness and accuracy. We have compared the convergence performance of gossip in different types of networks, say ring and mesh, and different sizes of networks. 

Section X Future work
Future work:
Improvements in retrieve-update
Current implementation uses write all read one strategy. It has performance overhead of writing data to all replicas for each write request. Thus we recommend following improvements as a future work 
1. Distributed Name node: Few nodes will contain mapping list of fragment id for each registered node. Although this implementation may suffer contention on name node. This can be implemented for faster writes by using write once and read latest strategy. 
2. Quorum based retrieve-read: This implementation uses mutual exclusion principle along with write once, read latest strategy. Which means while updating, data is written on any node in update quorum instead of all nodes which contains replica. On the other hand while reading, node selects most updated copy of data form quorum. This ensures less contention and better performance.

